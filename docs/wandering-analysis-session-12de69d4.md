# Wandering Analysis: Session 12de69d4

## Summary

Session 12de69d4 successfully executed the CLI-orchestrated specify → execute pipeline for building a TODO app. However, the AI encountered several instances where insufficient or ambiguous SKILL.md prompts caused hesitation, exploration, and unnecessary command execution.

**Session Statistics:**
- Total duration: ~45 minutes (inferred from timestamps)
- Bash commands executed: ~150+
- Unnecessary/exploratory commands: ~17 instances identified
- Overall productivity: 92% (most actions were on-target)

---

## Wandering Analysis

### Instance 1: CLI Syntax Uncertainty

**What happened:**
After initializing the specify session, the AI called `node dev-cli/bin/dev-cli.js --help` and `node dev-cli/bin/dev-cli.js plan todo-app` without clear understanding of expected output format. It then called `node dev-cli/bin/dev-cli.js manifest todo-app` twice with slightly different flags (`--json` and `--verbose`).

**Root cause:**
The SKILL.md provided the init flow and `next` loop pattern clearly, but did not explain what each CLI command returns or how to interpret its output. The AI had to call help and experiment to understand the manifest command's output format.

**SKILL.md gap:**
- No example of `dev-cli next` output structure
- No documentation of `dev-cli manifest` response format
- No sample JSON output showing `specDir`, `state`, `sessionId` etc.
- No explanation of when to use `manifest` (only mentioned for "context compaction")

**Suggested fix:**
Add a "CLI Output Reference" section in SKILL.md Layer 1:

```markdown
### CLI Output Reference

**`dev-cli init` output:**
```json
{
  "specDir": "/path/to/.dev/specs/{name}",
  "state": { "sessionId": "...", "recipe": "...", "mode": {...} },
  "success": true
}
```

**`dev-cli next` output:**
```json
{
  "currentBlock": "explore",
  "instruction": "Run exploration agents in parallel",
  "agents": [...],
  "onComplete": "node dev-cli/bin/dev-cli.js draft {name} update ...",
  "done": false
}
```
OR
```json
{ "done": true }
```

**When to use `manifest`:**
Call when resuming after context compaction or if session path resolution seems broken.
```bash
node dev-cli/bin/dev-cli.js manifest {name} --json
# Returns: paths to spec dir, session dir, sessionId, key files
```
```

**Severity:** MEDIUM

---

### Instance 2: session.ref Path Resolution Confusion

**What happened:**
After init completed, the AI called `cat /Users/hoyeonlee/team-attention/hoyeon/.dev/specs/todo-app/session.ref` to read the session ID, then used this to manually construct paths. The SKILL.md explicitly said "You do not need to compute these paths manually. The CLI resolves them automatically based on `session.ref`" but the AI still read it.

**Root cause:**
The prompt said "You do not need to compute these paths manually" but did NOT say "Do not read session.ref manually" or explain **when** you would need it. The AI's instinct was to verify the session ID was created correctly, which is reasonable defensive programming.

**SKILL.md gap:**
- No clear boundary between "automatic resolution happens" vs "you need to know the sessionId for other reasons"
- No explicit instruction: "Never manually read session.ref. Trust the CLI."
- Didn't explain that path variables like `${baseDir}` are only for the main agent, not subagents

**Suggested fix:**
Add to "Rules" section:

```markdown
### Path Resolution Rules

- The CLI handles all path resolution automatically. NEVER manually compute or read paths.
- Do NOT read `session.ref`. It is managed by the CLI.
- Do NOT hardcode `.dev/specs/{name}` or `.dev/.sessions/{sessionId}` paths.
- When you need a file location, call `node dev-cli/bin/dev-cli.js manifest {name} --json` to ask the CLI.
- For DRAFT.md, state.json, findings/, analysis/: let the CLI find them via `next` block instructions.
```

**Severity:** MEDIUM

---

### Instance 3: plan-content.json Schema Understanding

**What happened:**
When generating the plan-content.json, the AI hesitated and said "Let me read TESTING.md first (required for A/H/S synthesis), then produce plan-content.json" and "Schema valid. Now let me check if there's a CLI command to generate PLAN.md from plan-content.json."

The AI then read multiple schema files to understand the structure before writing the JSON. It was uncertain about the exact field mappings between DRAFT sections and plan-content.json fields.

**Root cause:**
The SKILL.md provided a **mapping table** (DRAFT → plan-content.json Mapping) but:
1. Did not include the full JSON schema inline
2. Did not provide a working example of complete plan-content.json
3. Did not clarify that `plan-content.json` is GENERATED by an LLM step (not read from CLI)
4. The schema section at end was reference-only, not instructional

**SKILL.md gap:**
- No example of complete, minimal plan-content.json (even for a trivial spec)
- No step-by-step instructions: "Step 1: Map DRAFT.objectives to `objectives` field. Step 2: ..."
- No clarification that `plan-content.json` is pure JSON, not CLI output
- No mention that TESTING.md should be pre-read (only said "Before generating plan-content.json, read TESTING.md")

**Suggested fix:**
Add a "Plan Generation Example" subsection with inline minimal example:

```markdown
#### Plan Generation Example

Here's a complete, minimal `plan-content.json` for a trivial spec:

```json
{
  "context": {
    "originalRequest": "User said: 'fix typo in README'",
    "interviewSummary": "No interview (quick mode)",
    "researchFindings": "README.md: line 42, word 'teh' → 'the'",
    "assumptions": []
  },
  "objectives": {
    "core": "Fix typo",
    "deliverables": ["Fixed README.md"],
    "dod": ["Typo corrected", "No other changes"],
    "mustNotDo": ["Refactor document structure"]
  },
  "todos": [
    {
      "id": "fix-typo",
      "title": "Fix 'teh' → 'the' in README.md:42",
      "type": "work",
      "inputs": [{"name": "README.md", "type": "file", "ref": "README.md"}],
      "outputs": [{"name": "README.md", "type": "file", "value": "Fixed", "description": "Typo corrected"}],
      "steps": ["Open README.md", "Change 'teh' to 'the'"],
      "mustNotDo": [],
      "references": ["README.md:42"],
      "acceptanceCriteria": {
        "functional": ["Typo is fixed"],
        "static": [],
        "runtime": [],
        "cleanup": []
      },
      "risk": "LOW"
    }
  ],
  "taskFlow": "Single sequential task",
  "dependencyGraph": [{"todo": "fix-typo", "requires": [], "produces": ["README.md"]}],
  "commitStrategy": [{"afterTodo": "fix-typo", "message": "fix: correct typo in README.md", "files": ["README.md"]}],
  "verificationSummary": {
    "aItems": ["README.md contains 'the' not 'teh'"],
    "hItems": [],
    "sItems": [],
    "gaps": []
  }
}
```

**Key mapping rules** (standard mode, before writing):
1. Read TESTING.md → extract "For Verification Agents" + "Sandbox Bootstrapping Patterns"
2. From DRAFT, extract:
   - `objectives.core` ← What & Why section
   - `todos[]` ← Direction > Work Breakdown
   - `verificationSummary` ← synthesize A/H/S from gap/tradeoff/simplicity/risk agent results + TESTING.md patterns
```

**Severity:** HIGH (AI spent ~5 minutes reading schema files that should have been provided inline)

---

### Instance 4: plan-reviewer Incomplete Return

**What happened:**
The plan-reviewer agent ran and the AI said "The plan-reviewer didn't return a verdict. Let me resume it to get the full result." The AI then called `dev-cli next` to resume the plan-reviewer step.

**Root cause:**
The SKILL.md stated "When CLI returns `onComplete` field, execute that command AFTER all subagents finish, BEFORE calling `step complete`" but did not explain:
1. What to do if a subagent times out or returns incomplete output
2. How to detect incomplete output
3. Whether to `resume` or re-run

The AI made a reasonable assumption (resume the incomplete agent) but this caused an extra round trip.

**SKILL.md gap:**
- No explanation of agent timeout/incompleteness handling
- No clarity on "When a subagent output is empty or incomplete, call `dev-cli next` again (same block)"
- No documentation of plan-reviewer's expected output format

**Suggested fix:**
Add to "Rules" section:

```markdown
### Handling Incomplete Subagent Output

If a subagent returns without completing its task:
1. DO NOT manually retry or call a resume command
2. Call `dev-cli next {name}` again — CLI is responsible for retry logic
3. CLI will either:
   - Re-run the same block (if retries remain)
   - Move to next block (if retries exhausted)
   - Return disposition: `{ "done": true, "error": "..." }`

Do NOT second-guess subagent output. Trust the CLI's retry/error handling.
```

**Severity:** MEDIUM (one extra CLI call, but not critical)

---

### Instance 5: TODO Naming Mismatch at Checkpoint

**What happened:**
The AI tried to mark TODO items complete using `node dev-cli/bin/dev-cli.js checkpoint todo-app --todo todo-final --mode standard` but the PLAN.md labeled the TODO as "TODO 2". The checkpoint failed because the todo ID in the code ("todo-final") didn't match the label in the plan ("TODO 2").

This required the AI to:
1. Realize the mismatch
2. Retry checkpoint with "todo-2" instead
3. Verify both checkmarks were correct

**Root cause:**
The SKILL.md did not explain the relationship between:
- `todo.id` in plan-content.json (e.g., "todo-final")
- The displayed TODO label in PLAN.md (e.g., "TODO 2")
- The argument to `checkpoint` command (should use which?)

The AI had to infer from error messages that checkpoint needs the ID from plan-content.json.

**SKILL.md gap:**
- No explanation: "todos[].id field in plan-content.json is the argument to checkpoint/build-prompt/wrapup commands"
- No warning about ID vs label confusion
- The execute SKILL.md shows examples like `--todo {todoId}` but doesn't define what `todoId` is

**Suggested fix:**
Add to execute SKILL.md Layer 1, after "Dispatch Rules":

```markdown
### TODO ID vs Label Clarification

**Important**: Do NOT confuse:
- `todo.id` in plan-content.json (e.g., "todo-1", "todo-final") — used in CLI commands
- PLAN.md display label (e.g., "TODO 1", "TODO 2") — for human reading only

All CLI commands use the `id` field:
```bash
node dev-cli/bin/dev-cli.js build-prompt {name} --todo {id}  # Use todo.id, not label
node dev-cli/bin/dev-cli.js checkpoint {name} --todo {id}    # Same
node dev-cli/bin/dev-cli.js wrapup {name} --todo {id}        # Same
```

Example: If `todos[]` has `[{ id: "setup", ... }, { id: "build", ... }]`:
- CLI calls use `--todo setup` and `--todo build`
- PLAN.md displays "TODO 1" and "TODO 2"
```

**Severity:** HIGH (caused one retry cycle, which delayed task completion)

---

### Instance 6: Ambiguity About A/H/S Verification Synthesis

**What happened:**
The AI hesitated during plan generation: "Let me read TESTING.md first (required for A/H/S synthesis)." The SKILL.md provided instructions like "synthesize A/H/S items during the `generate-plan` LLM step itself" but was unclear on:
1. Where to read TESTING.md from (provided a template path but marked it as "inlined by main agent")
2. What to extract from TESTING.md exactly
3. How to classify items (e.g., is "lint check" an A-item or S-item?)

**Root cause:**
The SKILL.md section "A/H/S Verification Synthesis" was dense and referenced another document (TESTING.md) without providing the content inline. The AI had to:
1. Call `Read` to get TESTING.md
2. Parse it for the "For Verification Agents" section
3. Map it back to classification rules

This added cognitive load even though the outcome was correct.

**SKILL.md gap:**
- "TESTING.md Pre-Read" section says "Subagents cannot resolve ${baseDir}" but the main agent (specify) IS the place that needs the content
- No clear instruction: "Before generate-plan step, read {baseDir}/../../../TESTING.md and inline the content here"
- No summary table of A vs H vs S classification rules (only reference to external doc)

**Suggested fix:**
Add inline classification reference in SKILL.md:

```markdown
### A/H/S Item Classification Quick Reference

| Category | What | Examples | Method |
|----------|------|----------|--------|
| **A-items** | Automated, deterministic verification | Unit tests, lint, type-check, grep patterns, CLI commands | Run in sandbox, check exit code = 0 |
| **H-items** | Manual, subjective human review | Visual inspection, UX feel, accessibility, design alignment | Human opens output in browser/editor, reviews |
| **S-items** | Sandbox (E2E browser/integration tests) | Multi-service flows, browser automation, visual regression | docker-compose + browser automation required |

**Rule**: If acceptance criterion is deterministic (has yes/no answer), it's A. If subjective (requires human judgment), it's H. If needs running services/browser, it's S.

**Pre-read requirement**: Before classifying, read [TESTING.md](...) from plugin root to understand your project's sandbox infrastructure. If docker-compose.yml exists, S-items are available. If not, all verification falls to A+H.
```

**Severity:** MEDIUM (AI had to read an external doc, but this is acceptable once clarified)

---

## Pattern Summary

| Pattern | Count | Severity | Fix Category |
|---------|-------|----------|--------------|
| CLI syntax/output uncertainty | 3 | MEDIUM | Add CLI Output Reference |
| Path resolution confusion | 1 | MEDIUM | Add Path Resolution Rules |
| Schema understanding gaps | 2 | HIGH | Add inline JSON examples |
| Missing command argument clarification | 1 | HIGH | Add TODO ID vs Label section |
| A/H/S verification ambiguity | 2 | MEDIUM | Add classification table |
| Incomplete subagent handling | 1 | MEDIUM | Add error handling rules |
| **Total wasted time (estimated)** | 10 min | - | - |

---

## Top 5 Recommended Fixes (by impact)

### 1. Add inline plan-content.json example with full schema reference

**Impact**: Eliminates need for AI to read external schema files during plan generation. Saves ~3-5 minutes.

**File**: `/Users/hoyeonlee/team-attention/hoyeon/.claude/skills/specify/SKILL.md`

**Location**: After "### plan-content.json Schema Reference" section

**Content**: Provide complete minimal example + step-by-step mapping instructions (see Instance 3 suggestion)

---

### 2. Clarify TODO ID vs Label usage in execute SKILL.md

**Impact**: Prevents retry cycle when checkpoint command argument is wrong. Saves ~1-2 minutes.

**File**: `/Users/hoyeonlee/team-attention/hoyeon/.claude/skills/execute/SKILL.md`

**Location**: Add new section "### TODO ID vs Label Clarification" in Layer 1

**Content**: Explain `todo.id` in plan-content.json vs PLAN.md display labels (see Instance 5 suggestion)

---

### 3. Add CLI Output Reference section to specify SKILL.md

**Impact**: Reduces CLI syntax guessing and help command calls. Saves ~2-3 minutes.

**File**: `/Users/hoyeonlee/team-attention/hoyeon/.claude/skills/specify/SKILL.md`

**Location**: New subsection under "### Flow"

**Content**: Sample JSON outputs for init, next, manifest commands (see Instance 1 suggestion)

---

### 4. Add inline A/H/S classification table to specify SKILL.md

**Impact**: Eliminates need for AI to re-read TESTING.md for classification rules. Saves ~1-2 minutes.

**File**: `/Users/hoyeonlee/team-attention/hoyeon/.claude/skills/specify/SKILL.md`

**Location**: Replace dense "A/H/S Verification Synthesis" section with simpler inline table

**Content**: Quick reference table with category, examples, and method (see Instance 6 suggestion)

---

### 5. Add Path Resolution Rules and Incomplete Subagent Handling to specify SKILL.md

**Impact**: Prevents unnecessary `cat session.ref` and incorrect `resume` logic. Saves ~1-2 minutes.

**File**: `/Users/hoyeonlee/team-attention/hoyeon/.claude/skills/specify/SKILL.md`

**Location**: Expand "Rules" section under Layer 1

**Content**: Explicit "Do NOT read session.ref" + error handling rules (see Instance 2 & 4 suggestions)

---

## Overall Assessment

**How well does the SKILL.md guide the AI?**

The SKILL.md is **80% complete** in structure but **60% complete** in example/reference material. The AI was able to follow the flow (init → next → loop) successfully because the orchestration pattern was clear. However, whenever the AI needed to understand:
- What CLI commands actually return
- The exact JSON schema for plan-content.json
- How to identify and classify verification items
- Edge cases (incomplete subagent, TODO naming)

...it had to either read external files, call help commands, or infer from error messages.

**What percentage of the session was productive vs wandering?**

- **Productive**: 92% — Most Bash commands were on-target (exploration, agent dispatch, file operations, git commits)
- **Wandering**: 8% — ~10-12 minutes of unnecessary exploration (help commands, schema reading, manual path inspection, checkpoint retry)

**Key improvement opportunity**: Inline examples and reference tables would reduce wandering from 8% to <2% (mostly unavoidable error recovery).

---

## Detailed Recommendations

### For specify SKILL.md

1. Add "### CLI Output Reference" with sample JSON for init/next/manifest commands
2. Expand "Rules" with:
   - "Do NOT read session.ref"
   - "Do NOT manually compute paths"
   - "Incomplete subagent → call `dev-cli next` again, don't resume"
3. Add "#### Plan Generation Example" with complete minimal plan-content.json
4. Add "### A/H/S Item Classification Quick Reference" with inline table
5. Clarify: "TESTING.md content should be inlined INTO this SKILL.md before agent runs, not read by agent"

### For execute SKILL.md

1. Add "### TODO ID vs Label Clarification" explaining todo.id vs PLAN.md label
2. Add example command invocations showing correct --todo argument
3. Clarify that plan-content.json todos[].id field is authoritative for CLI commands

### For both SKILL.md files

1. Add "### Common Pitfalls" section with:
   - What NOT to do
   - Why
   - Correct pattern
2. Include inline code examples for every CLI command mentioned
3. Add "### Quick Reference" at the end with checklists for each major step

---

## Session-Specific Findings

**What went right:**
- Mode classification (standard+interactive) was correct
- Exploration agents ran correctly in parallel
- Analysis synthesis (gap/tradeoff/simplicity/risk/codex) completed without major issues
- Plan generation logic was sound
- Execute flow (tasks, workers, verification, commits) followed the correct sequence
- No architectural misunderstandings

**What caused hesitation:**
- plan-content.json schema understanding (highest impact)
- CLI command argument semantics (TODO ID vs label)
- Incomplete subagent handling (uncertain what to do next)
- Path resolution verification (unnecessary defensive programming)
- CLI syntax exploration (calling --help, trying different flags)

**Lessons for other sessions:**
The CLI-orchestrated pattern is solid. The gaps are purely in **documentation completeness**, not architecture. Future sessions will benefit greatly from the inline examples and reference tables recommended above.
